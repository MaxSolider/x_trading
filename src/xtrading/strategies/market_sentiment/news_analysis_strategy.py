"""
çƒ­é—¨æ–°é—»åˆ†æç­–ç•¥
å¯¹æ–°é—»èµ„è®¯è¿›è¡Œåˆ†è¯è§£æå…³é”®è¯å¹¶è®¡ç®—å…³é”®è¯å‡ºç°é¢‘æ¬¡
"""

import pandas as pd
from typing import Dict, Any, Optional, List
from collections import Counter
import re

try:
    import jieba
    import jieba.analyse
    JIEBA_AVAILABLE = True
except ImportError:
    JIEBA_AVAILABLE = False
    print("âš ï¸ jiebaåº“æœªå®‰è£…ï¼Œæ–°é—»å…³é”®è¯åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚å®‰è£…å‘½ä»¤: pip install jieba")

from ...repositories.news_query import NewsQuery


class NewsAnalysisStrategy:
    """çƒ­é—¨æ–°é—»åˆ†æç­–ç•¥ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ–°é—»åˆ†æç­–ç•¥"""
        self.news_query = NewsQuery()
        # åˆå§‹åŒ–jiebaåˆ†è¯
        if JIEBA_AVAILABLE:
            jieba.initialize()
        # åŠ è½½åœç”¨è¯ï¼ˆå¦‚æœæœ‰åœç”¨è¯æ–‡ä»¶ï¼‰
        self.stopwords = self._load_stopwords()
        if JIEBA_AVAILABLE:
            print("âœ… çƒ­é—¨æ–°é—»åˆ†æç­–ç•¥åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âš ï¸ çƒ­é—¨æ–°é—»åˆ†æç­–ç•¥åˆå§‹åŒ–æˆåŠŸï¼ˆjiebaæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½ä¸å¯ç”¨ï¼‰")
    
    def _load_stopwords(self) -> set:
        """
        åŠ è½½åœç”¨è¯è¡¨
        
        Returns:
            set: åœç”¨è¯é›†åˆ
        """
        stopwords = set()
        # å¸¸è§çš„åœç”¨è¯
        common_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 
            'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ',
            'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰',
            'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ç­‰', 'ä¸', 'åŠ',
            'å¹´', 'æœˆ', 'æ—¥', 'æ—¶', 'åˆ†', 'ç§’', 'å¹´', 'æœˆ', 'æ—¥',
            'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©', 'ä»Šå¹´', 'æ˜å¹´', 'å»å¹´'
        }
        stopwords.update(common_stopwords)
        return stopwords
    
    def analyze_news_keywords(self, news_data: Optional[pd.DataFrame] = None, top_n: int = 50) -> Dict[str, Any]:
        """
        åˆ†ææ–°é—»å…³é”®è¯
        
        Args:
            news_data: æ–°é—»æ•°æ®DataFrameï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨è·å–
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯ï¼Œé»˜è®¤ä¸º50
            
        Returns:
            Dict[str, Any]: å…³é”®è¯åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
                keywords: List[Dict] å…³é”®è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'keyword': str, 'count': int}
                wordcloud_data: Dict ç”¨äºç”Ÿæˆè¯äº‘çš„æ•°æ®
                total_keywords: int å…³é”®è¯æ€»æ•°
                total_news: int æ–°é—»æ€»æ•°
        """
        try:
            if not JIEBA_AVAILABLE:
                print("âŒ jiebaåº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå…³é”®è¯åˆ†æ")
                return {
                    'keywords': [],
                    'wordcloud_data': {},
                    'total_keywords': 0,
                    'total_news': 0
                }
            
            # å¦‚æœæ²¡æœ‰æä¾›æ–°é—»æ•°æ®ï¼Œåˆ™è‡ªåŠ¨è·å–
            if news_data is None:
                print("ğŸ” æ­£åœ¨è·å–æ–°é—»æ•°æ®...")
                news_data = self.news_query.get_news()
            
            if news_data is None or news_data.empty:
                print("âš ï¸ æœªè·å–åˆ°æ–°é—»æ•°æ®")
                return {
                    'keywords': [],
                    'wordcloud_data': {},
                    'total_keywords': 0,
                    'total_news': 0
                }
            
            print(f"ğŸ“° å¼€å§‹åˆ†æ {len(news_data)} æ¡æ–°é—»çš„å…³é”®è¯...")
            
            # æå–æ‰€æœ‰æ–°é—»å†…å®¹
            content_list = news_data['å†…å®¹'].fillna('').astype(str).tolist()
            
            # åˆ†è¯å¹¶æå–å…³é”®è¯
            all_keywords = []
            for content in content_list:
                if content and content.strip():
                    # ä½¿ç”¨jiebaåˆ†è¯
                    words = self._extract_keywords(content)
                    all_keywords.extend(words)
            
            # ç»Ÿè®¡å…³é”®è¯é¢‘æ¬¡
            keyword_counter = Counter(all_keywords)
            
            # è·å–å‰Nä¸ªå…³é”®è¯
            top_keywords = keyword_counter.most_common(top_n)
            
            # æ„å»ºç»“æœ
            keywords_list = [
                {'keyword': keyword, 'count': count}
                for keyword, count in top_keywords
            ]
            
            # æ„å»ºè¯äº‘æ•°æ®ï¼ˆæ ¼å¼ï¼š{word: weight}ï¼‰
            wordcloud_data = {
                keyword: count
                for keyword, count in top_keywords
            }
            
            result = {
                'keywords': keywords_list,
                'wordcloud_data': wordcloud_data,
                'total_keywords': len(keyword_counter),
                'total_news': len(news_data)
            }
            
            print(f"âœ… å…³é”®è¯åˆ†æå®Œæˆï¼Œå…±æå– {result['total_keywords']} ä¸ªå”¯ä¸€å…³é”®è¯")
            return result
            
        except Exception as e:
            print(f"âŒ å…³é”®è¯åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'keywords': [],
                'wordcloud_data': {},
                'total_keywords': 0,
                'total_news': 0
            }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        try:
            if not JIEBA_AVAILABLE:
                return []
            
            # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ï¼‰
            text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
            
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.cut(text, cut_all=False)
            
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            keywords = []
            for word in words:
                word = word.strip()
                # è¿‡æ»¤æ¡ä»¶ï¼šé•¿åº¦>=2ï¼Œä¸æ˜¯åœç”¨è¯ï¼Œä¸æ˜¯çº¯æ•°å­—
                if (len(word) >= 2 and 
                    word not in self.stopwords and 
                    not word.isdigit() and
                    not re.match(r'^\d+$', word)):
                    keywords.append(word)
            
            return keywords
            
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬åˆ†è¯å¤±è´¥: {e}")
            return []
    
    def generate_keyword_frequency_table(self, keywords_list: List[Dict[str, Any]], max_rows: int = 30) -> pd.DataFrame:
        """
        ç”Ÿæˆå…³é”®è¯é¢‘æ¬¡è¡¨æ ¼
        
        Args:
            keywords_list: å…³é”®è¯åˆ—è¡¨
            max_rows: æœ€å¤§æ˜¾ç¤ºè¡Œæ•°
            
        Returns:
            DataFrame: å…³é”®è¯é¢‘æ¬¡è¡¨æ ¼
        """
        try:
            if not keywords_list:
                return pd.DataFrame(columns=['å…³é”®è¯', 'å‡ºç°é¢‘æ¬¡'])
            
            # æ„å»ºDataFrame
            df = pd.DataFrame(keywords_list)
            df.columns = ['å…³é”®è¯', 'å‡ºç°é¢‘æ¬¡']
            
            # é™åˆ¶è¡Œæ•°
            if len(df) > max_rows:
                df = df.head(max_rows)
            
            return df
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå…³é”®è¯é¢‘æ¬¡è¡¨æ ¼å¤±è´¥: {e}")
            return pd.DataFrame(columns=['å…³é”®è¯', 'å‡ºç°é¢‘æ¬¡'])

